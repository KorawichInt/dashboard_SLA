{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, types\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    # Load the 'metadata' column and convert JSON strings into a DataFrame\n",
    "    if 'metadata' in chunk.columns:\n",
    "        metadata_df = chunk['metadata'].apply(json.loads).apply(pd.Series)\n",
    "        \n",
    "        # Reorder columns based on the keys in the first metadata JSON object\n",
    "        columns_order = list(json.loads(chunk['metadata'].iloc[0]).keys())\n",
    "        \n",
    "        # Handle the 'created_date' field if it exists\n",
    "        if 'created_date' in metadata_df.columns:\n",
    "            metadata_df['created_date'] = metadata_df['created_date'].apply(lambda x: x['$date'] if isinstance(x, dict) and '$date' in x else x)\n",
    "        \n",
    "        # Combine the original chunk with the expanded metadata DataFrame\n",
    "        final_df = pd.concat([chunk.drop(columns=['metadata']), metadata_df], axis=1)\n",
    "        \n",
    "        # Reorder columns so that the metadata columns appear in the desired order\n",
    "        final_df = final_df.reindex(columns=columns_order + final_df.columns.difference(columns_order).tolist())\n",
    "\n",
    "        # Convert any dictionaries in the DataFrame to JSON strings\n",
    "        for col in final_df.columns:\n",
    "            if final_df[col].dtype == 'object':\n",
    "                final_df[col] = final_df[col].apply(lambda x: json.dumps(x) if isinstance(x, dict) else x)\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        return chunk\n",
    "\n",
    "def load_csv_to_postgresql(path, engine, chunksize=1000000):\n",
    "    i = 0\n",
    "    loaded_rows = 0\n",
    "\n",
    "    for chunk in pd.read_csv(path, chunksize=chunksize):\n",
    "        processed_chunk = process_chunk(chunk)\n",
    "        \n",
    "        # Define column types for PostgreSQL\n",
    "        dtype = {\n",
    "            'timestamp': types.TIMESTAMP(),\n",
    "            'state': types.INTEGER(),\n",
    "            'address': types.VARCHAR(),\n",
    "            'created_date': types.TIMESTAMP(),\n",
    "            'downtime': types.BOOLEAN(),\n",
    "            'duration': types.INTEGER(),\n",
    "            'groups': types.VARCHAR(),  # JSON or TEXT depending on the structure\n",
    "            'host_name': types.VARCHAR(),\n",
    "            'id': types.VARCHAR(),\n",
    "            'labels': types.VARCHAR(),  # JSON or TEXT depending on the structure\n",
    "            'name': types.VARCHAR(),\n",
    "        }\n",
    "\n",
    "        # Append processed data to PostgreSQL table with specified data types\n",
    "        processed_chunk.to_sql(name='host_states_cleaned', con=engine, if_exists='append', index=False, dtype=dtype)\n",
    "        \n",
    "        loaded_rows += len(processed_chunk)\n",
    "        i += 1\n",
    "        print(f\"Processed and loaded chunk #{i} ({loaded_rows} rows) to database.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your PostgreSQL connection string\n",
    "    connection_string = 'postgresql://<username>:<password>@<host>:<port>/<dbname>'\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Path to your CSV file\n",
    "    path = 'D:/datasets/host_states.csv'\n",
    "    \n",
    "    # Load data from CSV to PostgreSQL\n",
    "    load_csv_to_postgresql(path, engine, chunksize=1000000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
